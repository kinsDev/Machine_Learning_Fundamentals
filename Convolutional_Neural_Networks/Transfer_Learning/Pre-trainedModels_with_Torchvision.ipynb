{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Getting a Pre-Trained Model with torchvision"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5d290ac2ccc5363"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# For example, let's load a resnet architecture\n",
    "import torchvision.models\n",
    "model = torchvision.models.resnet18(pretrained = True)\n",
    "\n",
    "# Setting pretrained to True means that we want to i=obtained the weights that were training on ImageNet or some other Dataset.\n",
    "\n",
    "# Setting Pretrained to False means that we want to train ResNet from scratch."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd94004ef0412f68"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Freezing and Thawing Layers and Parameters\n",
    "\n",
    "- A frozen parameter is a parameter that is not allowed to vary during training. In other words, backpropagation will ignore that parameter and won't change its value nor compute the gradient of the loss with respect to that parameter."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1130d6df1e989a88"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# In PyTorch you can freeze all the parameters of a network using the following code:\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "def258f5af6a0345"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Similarly, you can also freeze the parameters of a single layer. For example, say that this layer is called fc, then:\n",
    "\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = False"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e3290d40e20e807"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- You can instead thaw parameters that are frozen by instead setting:\n",
    "```\n",
    "param.requires_grad = True\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be2599908b80e9e2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BatchNorm\n",
    "\n",
    "- The BatchNorm layer is a special case: it has two parameters (gamma and beta), but it also has two buffers that are used to accumulate the mean and standard deviation of the dataset during training.\n",
    "- If you only use requires_grad=False then you are only fixing gamma and beta. The statistics about the dataset are still accumulated. Sometimes fixing those as well can help the performance, but not always. Experimentation, as usual, is key.\n",
    "- If we want to also freeze the statistics accumulated we need to put the entire layer in evaluation mode by using eval (instead of requires_grad=False for its parameters):```model.bn.eval()```\n",
    "- Note that this is different than using ```model.eval()``` (which would put the entire model in evaluation mode). You can invert this operation by putting the BatchNorm layer back into training mode: ```model.bn.train()```. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d19985130de63577"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualizing an Architecture with Netron"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5efd34cf47d37f8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# First we need to export the model, for example:\n",
    "\n",
    "random_image = torch.rand((1, 3, 224, 224))\n",
    "\n",
    "scripted = torch.jit.trace(model, random_image)\n",
    "torch.jit.save(scripted, 'my_network.pt')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9327d28dd31b51b4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "torchtensor",
   "language": "python",
   "display_name": "TorchTensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
